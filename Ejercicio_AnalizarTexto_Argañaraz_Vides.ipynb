{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejercicio: Analizar texto\n",
    "Utilizar un texto de proyecto Gutenberg en castellano\n",
    "http://www.gutenberg.org/browse/languages/es\n",
    "\n",
    "Contar palabras y ordenar por frecuencia\n",
    "\n",
    "Limpiar preludio y licencia de Project Gutenberg\n",
    "\n",
    "Omitir “palabras vacías” (stop words) y símbolos\n",
    "\n",
    "Encontrar personajes\n",
    "\n",
    "Hacer un análisis extra a gusto\n",
    "\n",
    "Hint: Tutorial para español\n",
    "https://relopezbriega.github.io/blog/2017/09/23/procesamiento-del-lenguaje-natural-con-python/\n",
    "○ Usa spacy.io y github.com/chartbeat-labs/textacy\n",
    "○ No hace falta la parte de Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando las librerías que vamos a utilizar\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import textacy\n",
    "from textacy.datasets import Wikipedia\n",
    "from collections import Counter, defaultdict\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import spacy\n",
    "\n",
    "# graficos incrustados\n",
    "%matplotlib inline\n",
    "\n",
    "# función auxiliar\n",
    "def leer_texto(texto):\n",
    "    \"\"\"Funcion auxiliar para leer un archivo de texto\"\"\"\n",
    "    with open(texto, 'r', encoding=\"utf8\") as text:\n",
    "        return text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cargando el modelo en español de spacy\n",
    "nlp = textacy.load_spacy(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de caracteres del texto:\n",
      "313032\n",
      "Los primeros 100 caracteres:\n",
      "Project Gutenberg's Cuentos de Amor de Locura y de Muerte, by Horacio Quiroga\n",
      "\n",
      "This eBook is for the use of anyone anywhere at no cost and with\n",
      "almost no restrictions whatsoever.  You may copy it, gi\n",
      "Los ultimos 100 caracteres:\n",
      "rg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n",
      "Los ultimos 100 caracteres again:\n",
      "rg-tm,\n",
      "including how to make donations to the Project Gutenberg Literary\n",
      "Archive Foundation, how to help produce our new eBooks, and how to\n",
      "subscribe to our email newsletter to hear about new eBooks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Procesando un texto \n",
    "# Procesando 1984 de George Orwell - mi novela favorita\n",
    "texto = leer_texto('CuentosQuiroga.txt')\n",
    "print(\"Cantidad de caracteres del texto:\")\n",
    "print(len(texto))\n",
    "print(\"Los primeros 100 caracteres:\")\n",
    "print(texto[1:200])\n",
    "print(\"Los ultimos 100 caracteres:\")\n",
    "print(texto[len(texto)-200:len(texto)])\n",
    "print(\"Los ultimos 100 caracteres again:\")\n",
    "print(texto[-200:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacar preludio y licencia de final de documento\n",
    "sep1 =\"#Cuentos de Amor de Locura y de Muerte#\"\n",
    "sep2 =\"FIN\"\n",
    "head, sep1, tail = texto.partition(sep1)\n",
    "texto= sep1+tail\n",
    "head, sep2, tail = texto.partition(sep2)\n",
    "texto= head+sep2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cuentos de Amor de Locura y de Muerte#\\n\\n\\nHORACIO QUIROGA\\n\\n1917\\n\\n\\n\\n\\n#INDICE#\\n\\n\\nUna estación de amor\\nLos ojos sombríos\\nEl solitario\\nLa muerte de Isolda\\nEl infierno artificial\\nLa gallina degollada\\nLos b'"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Primeros 200 caracteres\n",
    "texto[1:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'é si a mucho más de cinco\\ncentímetros.\\n\\n--¿Es verdad?--murmura--o arrulla, mejor dicho.\\n\\n--¿Se puede poner arrulla?--le pregunto.\\n\\n--¡Sí, y esto, y esto! Y me da un beso.\\n\\n¿Qué más puedo añadir?\\n\\n\\nFIN'"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Últimos 200 caracteres\n",
    "texto[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hago esto, pongo todas las palabtas en minuscula\n",
    "#texto=texto.lower()  \n",
    "#Luego el nlp ya no puede distinguir ents del tipo PERSON 'PER'\n",
    "#Últimos 200 caracteres\n",
    "#texto[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_procesado = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encontrar_personajes(doc):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de los personajes de un `doc` con su cantidad de\n",
    "    ocurrencias\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :return: Lista de Tuplas con la forma\n",
    "        [('winston', 686), (\"o'brien\", 135), ('julia', 85),]\n",
    "    \"\"\"\n",
    "    \n",
    "    personajes = Counter()\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == 'PER': #Cuando usaba =='PERSON' no funcionaba\n",
    "            personajes[ent.lemma_] += 1\n",
    "            #print(\"Encontro un personaje\")\n",
    "            \n",
    "    return personajes.most_common() #se puede colocar en most_common(n), donde muestra los n personajes mas comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nébel', 49), ('', 44), ('Podeley', 21), ('Cayé', 20), ('María Elvira', 18), ('Candiyú', 17), ('Mazzini', 16), ('Berta', 13), ('Kassim', 13), ('Luis María', 12), ('Cooper', 11), ('Fué', 11), ('Así', 10), ('Benincasa', 9), ('Castelhum', 8), ('Alfonso', 8), ('Después', 6), ('Prince', 6), ('Míster Jones', 6), ('Inés', 6), ('Allí', 6), ('Volví', 6), ('Había', 6), ('Alicia', 6), ('Jones', 5), ('Pasó', 5), ('Milk', 5), ('Zapiola', 5), ('María', 4), ('Fragoso', 4), ('Octavio', 4), ('Jordán', 4), ('¡ Ah !', 4), ('Posadas', 4), ('Silex _', 4), ('María Elvira Funes', 4), ('Funes', 3), ('Y', 3), ('Miró', 3), ('¡ Pero', 3), ('Mi', 3), ('Wagner', 3), ('Ayestarain', 3), ('¡ Mi', 3), ('Vd', 3), ('¡ No', 3), ('Acaso', 3), ('Luis \\n María', 3), ('Tenía', 3), ('San Ignacio', 3), ('-PRON-', 3), ('Aquí', 2), ('¡ Bah !', 2), ('Allá', 2), ('Barigüí', 2), ('Me', 2), ('Entretanto', 2), ('Dije', 2), ('Está', 2), ('¡ Ah', 2), ('Qué', 2), ('Salimos', 2), ('Dejó', 2), ('¡ Todos', 2), ('Padilla', 2), ('Tarde', 2), ('Entró', 2), ('Dick', 2), ('¡ Qué', 2), ('Paranaí', 2), ('Volvió', 2), ('Miré', 2), ('Paraná', 2), ('Ustedes', 2), ('Durante', 2), ('Vieron', 2), ('Angélica', 2), ('Esto', 2), ('Mamá', 2), ('Espero', 2), ('¡ Quieren', 2), ('Fuí', 2), ('Pasé', 2), ('Recorrí', 2), ('Nació', 2), ('Cuándo', 2), ('María Margarita', 2), ('Su', 2), ('¡ Allí', 2), ('Corrió', 2), ('Jicky', 2), ('Elena', 2), ('Hall', 2), ('Al', 2), ('Sabía', 2), ('Yerba Company', 1), ('Luis María .', 1), ('Babeantes', 1), ('¡ Eduardo', 1), ('El Paraná', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(encontrar_personajes(texto_procesado)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def palabras_frecuentes(doc):\n",
    "    \"\"\"\n",
    "    Devuelve una lista de las palabras de un `doc` con su cantidad de\n",
    "    ocurrencias\n",
    "    \n",
    "    :param doc: NLP documento parseado por Spacy\n",
    "    :return: Lista de Tuplas con la forma\n",
    "        [('winston', 686), (\"o'brien\", 135), ('julia', 85),]\n",
    "    \"\"\"\n",
    "    \n",
    "    palabras = Counter()\n",
    "    for token in doc:\n",
    "            palabras[token.orth_] += 1\n",
    "            \n",
    "    return palabras.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 4341),\n",
       " ('\\n', 3341),\n",
       " ('.', 2611),\n",
       " ('de', 2239),\n",
       " ('\\n\\n', 1641),\n",
       " ('la', 1480),\n",
       " ('y', 1247),\n",
       " ('el', 1236),\n",
       " ('que', 1224),\n",
       " ('a', 1183),\n",
       " ('en', 1050),\n",
       " ('un', 631),\n",
       " ('su', 627),\n",
       " ('se', 610),\n",
       " ('no', 582),\n",
       " ('con', 573),\n",
       " ('!', 535),\n",
       " ('los', 530),\n",
       " ('...', 423),\n",
       " ('una', 363),\n",
       " ('al', 354),\n",
       " ('por', 354),\n",
       " ('lo', 350),\n",
       " ('--', 345),\n",
       " ('¡', 341),\n",
       " ('más', 320),\n",
       " ('las', 319),\n",
       " ('del', 318),\n",
       " ('me', 293),\n",
       " ('?', 277),\n",
       " ('para', 241),\n",
       " (';', 214),\n",
       " ('había', 213),\n",
       " ('¿', 204),\n",
       " (':', 201),\n",
       " ('mi', 192),\n",
       " ('*', 185),\n",
       " ('como', 178),\n",
       " ('Y', 176),\n",
       " ('es', 170),\n",
       " ('El', 170),\n",
       " ('sus', 168),\n",
       " ('Pero', 164),\n",
       " ('le', 163),\n",
       " ('sin', 158),\n",
       " ('ojos', 149),\n",
       " ('      ', 148),\n",
       " ('sobre', 138),\n",
       " ('No', 136),\n",
       " ('pero', 136),\n",
       " ('vez', 135),\n",
       " ('ya', 134),\n",
       " ('La', 121),\n",
       " ('todo', 119),\n",
       " ('era', 115),\n",
       " ('dos', 114),\n",
       " ('aún', 111),\n",
       " ('bien', 108),\n",
       " ('Nébel', 108),\n",
       " ('ella', 106),\n",
       " ('él', 105),\n",
       " ('día', 103),\n",
       " ('noche', 102),\n",
       " ('qué', 101),\n",
       " ('cuando', 101),\n",
       " ('yo', 100),\n",
       " ('fin', 94),\n",
       " ('María', 93),\n",
       " ('hasta', 90),\n",
       " ('si', 90),\n",
       " ('mí', 82),\n",
       " ('o', 79),\n",
       " ('después', 78),\n",
       " ('muy', 76),\n",
       " ('momento', 76),\n",
       " ('En', 75),\n",
       " ('hombre', 74),\n",
       " ('cabeza', 74),\n",
       " ('casa', 73),\n",
       " ('poco', 73),\n",
       " ('Los', 71),\n",
       " ('entonces', 71),\n",
       " ('Al', 69),\n",
       " ('_', 67),\n",
       " ('#', 66),\n",
       " ('nada', 66),\n",
       " ('esa', 66),\n",
       " ('mujer', 66),\n",
       " ('te', 64),\n",
       " ('madre', 64),\n",
       " ('nuevo', 61),\n",
       " ('fué', 61),\n",
       " ('siempre', 61),\n",
       " ('mismo', 61),\n",
       " ('tres', 60),\n",
       " ('ese', 60),\n",
       " ('amor', 60),\n",
       " ('?--', 60),\n",
       " ('.--', 60),\n",
       " ('ha', 60),\n",
       " ('días', 59),\n",
       " ('dijo', 58),\n",
       " ('-', 57),\n",
       " ('otro', 56),\n",
       " ('eso', 56),\n",
       " ('esto', 56),\n",
       " ('tarde', 54),\n",
       " ('usted', 54),\n",
       " ('--No', 54),\n",
       " ('años', 53),\n",
       " ('mucho', 52),\n",
       " ('Lidia', 52),\n",
       " ('también', 52),\n",
       " ('tenía', 52),\n",
       " ('A', 51),\n",
       " ('tras', 50),\n",
       " ('monte', 50),\n",
       " ('rato', 50),\n",
       " ('esta', 50),\n",
       " ('otra', 50),\n",
       " ('perros', 50),\n",
       " ('lado', 50),\n",
       " ('estaba', 49),\n",
       " ('ni', 49),\n",
       " ('así', 49),\n",
       " ('está', 49),\n",
       " ('voz', 49),\n",
       " ('toda', 48),\n",
       " ('tiempo', 48),\n",
       " ('ahora', 48),\n",
       " ('desde', 48),\n",
       " ('sí', 47),\n",
       " ('mañana', 47),\n",
       " ('vida', 46),\n",
       " ('entre', 46),\n",
       " ('hora', 46),\n",
       " ('aquí', 46),\n",
       " ('todos', 46),\n",
       " ('--Sí', 46),\n",
       " ('casi', 45),\n",
       " ('bajo', 45),\n",
       " ('mamá', 45),\n",
       " ('mano', 44),\n",
       " ('Por', 44),\n",
       " ('antes', 43),\n",
       " ('pues', 43),\n",
       " ('he', 43),\n",
       " ('nos', 43),\n",
       " ('mientras', 43),\n",
       " ('mirada', 41),\n",
       " ('horas', 41),\n",
       " ('hay', 41),\n",
       " (',--', 40),\n",
       " ('ser', 40),\n",
       " ('seguida', 39),\n",
       " ('Su', 39),\n",
       " ('miró', 39),\n",
       " ('perro', 39),\n",
       " ('veces', 38),\n",
       " ('agua', 38),\n",
       " ('cuanto', 38),\n",
       " ('cuatro', 38),\n",
       " ('allí', 38),\n",
       " ('Se', 37),\n",
       " ('aquella', 37),\n",
       " ('iba', 37),\n",
       " ('hacia', 37),\n",
       " ('ante', 37),\n",
       " ('habían', 37),\n",
       " ('caballos', 37),\n",
       " ('Elvira', 37),\n",
       " ('instante', 36),\n",
       " ('modo', 36),\n",
       " ('\\n\\n       ', 36),\n",
       " ('porque', 36),\n",
       " ('sé', 36),\n",
       " ('pronto', 36),\n",
       " ('Qué', 36),\n",
       " ('Me', 35),\n",
       " ('diez', 35),\n",
       " ('sino', 35),\n",
       " ('uno', 35),\n",
       " ('camino', 34),\n",
       " ('Kassim', 34),\n",
       " ('hacer', 34),\n",
       " ('cosa', 34),\n",
       " ('sido', 34),\n",
       " ('volvió', 34),\n",
       " ('siguiente', 34),\n",
       " ('algo', 34),\n",
       " ('De', 33),\n",
       " ('aquel', 33),\n",
       " ('luego', 33),\n",
       " ('tiene', 33),\n",
       " ('aunque', 33),\n",
       " ('Es', 32),\n",
       " ('fiebre', 32),\n",
       " ('largo', 32),\n",
       " ('sol', 32),\n",
       " ('menos', 32)]"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Frecuencias de palabras\n",
    "freq_palabras = palabras_frecuentes(texto_procesado)[:200]\n",
    "freq_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mios', 'antano', 'principalmente', 'uso', 'incluso', 'menudo', 'las', 'yo', 'ahora', 'creo', 'aqui', 'decir', 'desde', 'expresó', 'quedó', 'tan', 'gueno', 'sería', 'agregó', 'aquello', 'ser', 'ésos', 'dia', 'saber', 'tercera', 'quiere', 'arribaabajo', 'intento', 'podriamos', 'fui', 'ahí', 'conseguimos', 'solamente', 'poner', 'final', 'empleais', 'habla', 'quien', 'consigues', 'nuestras', 'usa', 'encuentra', 'mi', 'tanto', 'señaló', 'mejor', 'excepto', 'podriais', 'al', 'hacen', 'muchas', 'deprisa', 'trabajo', 'dijo', 'atras', 'tambien', 'podrán', 'días', 'hacerlo', 'sean', 'unos', 'próximos', 'buenas', 'esta', 'ampleamos', 'hacia', 'trabaja', 'otra', 'estamos', 'aquellas', 'en', 'ayer', 'vaya', 'hoy', 'para', 'nuestro', 'algunos', 'ese', 'no', 'nuestros', 'tenido', 'quiza', 'hecho', 'dijeron', 'próximo', 'realizar', 'pocos', 'estoy', 'si', 'trabajar', 'veces', 'bien', 'haber', 'este', 'cuatro', 'quizas', 'estas', 'primero', 'pronto', 'quiénes', 'nuevas', 'nosotros', 'sera', 'pues', 'mismos', 'mío', 'saben', 'solo', 'despacio', 'aquí', 'trabajas', 'emplean', 'vamos', 'parte', 'pasada', 'haces', 'tuyos', 'cual', 'podrian', 'casi', 'fue', 'intentas', 'adelante', 'estan', 'cualquier', 'nosotras', 'estaban', 'mia', 'todavía', 'sino', 'siendo', 'qué', 'dentro', 'ex', 'consiguen', 'cómo', 'él', 'usamos', 'dónde', 'ahi', 'mencionó', 'más', 'lejos', 'tal', 'mí', 'nuestra', 'segundo', 'estará', 'cuantos', 'suya', 'estos', 'debe', 'paìs', 'podrias', 'porque', 'ningunas', 'además', 'propias', 'sí', 'te', 'peor', 'eres', 'ningún', 'tiempo', 'llevar', 'sigue', 'les', 'apenas', 'hacemos', 'esto', 'os', 'vuestras', 'esa', 'detrás', 'voy', 'ya', 'afirmó', 'tuyas', 'arriba', 'sola', 'diferente', 'vosotros', 'buen', 'ninguna', 'hace', 'intentar', 'segun', 'añadió', 'nadie', 'breve', 'ellos', 'habia', 'llegó', 'cuáles', 'modo', 'ante', 'mio', 'todas', 'ustedes', 'pasado', 'otros', 'esas', 'una', 'bajo', 'estaba', 'horas', 'suyas', 'había', 'algún', 'durante', 'sabeis', 'junto', 'varios', 'dicen', 'primer', 'propia', 'sus', 'sido', 'tarde', 'estar', 'tenemos', 'primeros', 'través', 'verdadera', 'últimos', 'mas', 'eran', 'usas', 'podemos', 'quienes', 'supuesto', 'tendrá', 'cuántas', 'ningunos', 'intentan', 'pueda', 'sois', 'su', 'podría', 'última', 'queremos', 'nuevo', 'sin', 'vosotras', 'delante', 'éstas', 'como', 'adrede', 'el', 'dado', 'sabes', 'son', 'todos', 'mucho', 'deben', 'propios', 'aún', 'aquellos', 'actualmente', 'mal', 'posible', 'usted', 'ello', 'pesar', 'qeu', 'lo', 'gran', 'trata', 'han', 'están', 'hacer', 'solos', 'ninguno', 'ellas', 'antes', 'contra', 'aun', 'cinco', 'cuantas', 'tener', 'ver', 'teneis', 'día', 'ti', 'pais', 'habían', 'enseguida', 'por', 'usar', 'aunque', 'bueno', 'hablan', 'momento', 'así', 'poca', 'tras', 'ciertas', 'tengo', 'trabajais', 'estados', 'solas', 'allí', 'ademas', 'es', 'medio', 'empleas', 'estuvo', 'cierto', 'sé', 'según', 'ésa', 'acuerdo', 'mis', 'diferentes', 'conmigo', 'puedo', 'verdadero', 'tienen', 'sobre', 'vuestra', 'cierta', 'repente', 'realizado', 'nunca', 'cosas', 'sólo', 'ciertos', 'dan', 'ocho', 'tuvo', 'intentais', 'me', 'aquéllos', 'pocas', 'alguna', 'trabajan', 'he', 'vez', 'tuyo', 'uno', 'quizás', 'cuenta', 'donde', 'muchos', 'pueden', 'manera', 'comentó', 'serán', 'indicó', 'entre', 'hay', 'podrá', 'mediante', 'mayor', 'da', 'menos', 'podrían', 'raras', 'nueva', 'intenta', 'eramos', 'buenos', 'hizo', 'manifestó', 'temprano', 'otro', 'hubo', 'demás', 'la', 'existe', 'sabemos', 'aquélla', 'enfrente', 'somos', 'dio', 'ella', 'buena', 'respecto', 'detras', 'ha', 'siete', 'unas', 'nuevos', 'cuántos', 'ésta', 'toda', 'cada', 'salvo', 'valor', 'tampoco', 'cuánto', 'considera', 'existen', 'aseguró', 'mías', 'debido', 'nos', 'último', 'esos', 'pudo', 'que', 'tiene', 'consigue', 'cuanta', 'demasiado', 'está', 'ir', 'cuales', 'ése', 'grandes', 'fuimos', 'podeis', 'del', 'podria', 'puede', 'también', 'varias', 'realizó', 'últimas', 'lado', 'cuál', 'dice', 'van', 'misma', 'propio', 'míos', 'vuestros', 'alli', 'hasta', 'tu', 'con', 'parece', 'vais', 'informo', 'embargo', 'trabajamos', 'asi', 'proximo', 'dos', 'hago', 'tenía', 'cuándo', 'tú', 'ésas', 'luego', 'mientras', 'éstos', 'general', 'informó', 'igual', 'cerca', 'claro', 'se', 'ejemplo', 'partir', 'dicho', 'bastante', 'emplear', 'mucha', 'sabe', 'un', 'fuera', 'seis', 'dias', 'aquéllas', 'mía', 'mismo', 'de', 'algunas', 'será', 'total', 'aproximadamente', 'soyos', 'habrá', 'los', 'soy', 'conseguir', 'consigo', 'éste', 'cuanto', 'debajo', 'despues', 'dejó', 'dar', 'tendrán', 'estado', 'suyo', 'tenga', 'otras', 'era', 'cuánta', 'explicó', 'tus', 'intentamos', 'entonces', 'haciendo', 'sea', 'empleo', 'primera', 'lleva', 'pero', 'muy', 'consideró', 'haceis', 'eso', 'todo', 'contigo', 'siempre', 'dieron', 'siguiente', 'antaño', 'haya', 'nada', 'hicieron', 'mismas', 'poder', 'quién', 'conocer', 'le', 'alrededor', 'algo', 'aquél', 'encima', 'tres', 'tuya', 'quizá', 'aquel', 'eras', 'lugar', 'segunda', 'ambos', 'mias', 'cuando', 'va', 'anterior', 'hemos', 'fueron', 'estais', 'después', 'ni', 'ultimo', 'usan', 'vuestro', 'largo', 'aquella', 'fin', 'verdad', 'poco', 'todavia', 'usais', 'alguno'}\n"
     ]
    }
   ],
   "source": [
    "#Buscamos los stop words\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "print(STOP_WORDS)\n",
    "#stop = list(nlp.STOP_WORDS)\n",
    "#stop[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65106\n",
      "65106\n",
      "rmura -- o arrulla , . \n",
      "\n",
      " --¿Se arrulla ?-- pregunto . \n",
      "\n",
      " --¡Sí , y , y ! Y beso . \n",
      "\n",
      " ¿ añadir ? \n",
      "\n",
      "\n",
      "\n",
      "208911\n"
     ]
    }
   ],
   "source": [
    "#Esta es una manera de filtrar el texto, para luego aplicarle la libreria spacy al texto, y seguir extrayendo informacion.\n",
    "#En este caso, solo le extrajimos los STOP_WORDS. Pero tambien podriamos haber procedido de la misma manera para extraer \n",
    "#los simbolos, etc. Al final de este trabajo, procedemos de otra manera para extraer simbolos.\n",
    "texto_filtrado=\"\"\n",
    "print(len(texto_procesado))\n",
    "s=0\n",
    "for token in texto_procesado:\n",
    "    s=s+1\n",
    "    #print(token)\n",
    "    palabra=str(token)\n",
    "    #palabra=palabra.lower()\n",
    "    if palabra.lower() not in STOP_WORDS:\n",
    "        texto_filtrado = texto_filtrado +\" \"+ palabra \n",
    "\n",
    "\n",
    "print(s)\n",
    "print(texto_filtrado[-100:])\n",
    "print(len(texto_filtrado))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_procesado_filtrado = nlp(texto_filtrado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto sin filtrar\n",
      "[('Nébel', 49), ('', 44), ('Podeley', 21), ('Cayé', 20), ('María Elvira', 18), ('Candiyú', 17), ('Mazzini', 16), ('Berta', 13), ('Kassim', 13), ('Luis María', 12), ('Cooper', 11), ('Fué', 11), ('Así', 10), ('Benincasa', 9), ('Castelhum', 8), ('Alfonso', 8), ('Después', 6), ('Prince', 6), ('Míster Jones', 6), ('Inés', 6)]\n",
      "Texto con filtrado de STOP_WORDS\n",
      "[('Nébel', 60), ('Podeley', 25), ('Cayé', 23), ('María Elvira', 18), ('Candiyú', 17), ('Mazzini', 16), ('Berta', 15), ('Luis María', 12), ('María', 12), ('Kassim', 11), ('Benincasa', 11), ('Fué', 11), ('Cooper', 10), ('Inés', 10), ('Paraná', 9), ('Alfonso', 9), ('Funes', 9), ('Jones', 8), ('Posadas', 8), ('', 7)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Texto sin filtrar\")\n",
    "print(encontrar_personajes(texto_procesado)[:20])\n",
    "print(\"Texto con filtrado de STOP_WORDS\")\n",
    "print(encontrar_personajes(texto_procesado_filtrado)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 3341),\n",
       " ('.', 2611),\n",
       " ('\\n\\n', 1641),\n",
       " ('y', 1247),\n",
       " ('a', 1183),\n",
       " ('!', 535),\n",
       " ('...', 423),\n",
       " ('--', 345),\n",
       " ('¡', 341),\n",
       " ('?', 277),\n",
       " (';', 214),\n",
       " ('¿', 204),\n",
       " (':', 201),\n",
       " ('*', 185),\n",
       " ('Y', 176),\n",
       " ('El', 170),\n",
       " ('Pero', 164),\n",
       " ('ojos', 149),\n",
       " ('      ', 148)]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtro las palabras que no son stop words\n",
    "x=[(word, count) for word, count in freq_palabras if word not in STOP_WORDS]\n",
    "x[1:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Pero', 164),\n",
       " ('ojos', 149),\n",
       " ('      ', 148),\n",
       " ('Nébel', 108),\n",
       " ('noche', 102),\n",
       " ('María', 93),\n",
       " ('hombre', 74),\n",
       " ('cabeza', 74),\n",
       " ('casa', 73),\n",
       " ('mujer', 66),\n",
       " ('madre', 64),\n",
       " ('amor', 60),\n",
       " ('--No', 54),\n",
       " ('años', 53),\n",
       " ('Lidia', 52),\n",
       " ('monte', 50),\n",
       " ('rato', 50),\n",
       " ('perros', 50),\n",
       " ('mañana', 47),\n",
       " ('vida', 46),\n",
       " ('hora', 46),\n",
       " ('--Sí', 46),\n",
       " ('mamá', 45),\n",
       " ('mano', 44),\n",
       " ('mirada', 41),\n",
       " ('seguida', 39),\n",
       " ('miró', 39),\n",
       " ('perro', 39),\n",
       " ('agua', 38),\n",
       " ('caballos', 37),\n",
       " ('Elvira', 37),\n",
       " ('instante', 36),\n",
       " ('\\n\\n       ', 36),\n",
       " ('diez', 35),\n",
       " ('camino', 34),\n",
       " ('Kassim', 34),\n",
       " ('cosa', 34),\n",
       " ('volvió', 34),\n",
       " ('fiebre', 32)]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filtro las palabras de longitud mayor que 3, para remover los simbolos y palabras cortas\n",
    "y=[(word, count) for word, count in x if len(word)>3]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
